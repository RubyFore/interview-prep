{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453ac3d7",
   "metadata": {},
   "source": [
    "## Explain the 'bias-variance' trade off\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ec913",
   "metadata": {},
   "source": [
    "The bias-variance trade off refers to two important aspects of an estimate's correctness. Bias measures accuracy of the estimate, while the variance of an estimate measures the precision of estimates over repeated attempts at estimation. They measure different aspects of the estimator (or statistic) and optimizing for one of bias or variance may impact the other detrimentally. \n",
    "\n",
    "Mean Squared Error can decompose into $MSE(\\hat{\\theta}) = Var_\\theta(\\hat{\\theta}) - Bias(\\theta, \\hat{\\theta})^2$, so using MSE loss takes both variance and bias into account. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de123448",
   "metadata": {},
   "source": [
    "## Performance metrics for classification tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf780a5",
   "metadata": {},
   "source": [
    "#### Precision: \n",
    "\n",
    "What proportion of positive identifications was actually correct?\n",
    "\n",
    "$\\frac{TP}{TP+FP}$\n",
    "\n",
    "(aka positive predictive value)\n",
    "\n",
    "#### Recall: \n",
    "What proportion of actual positives was identified correctly?\n",
    "\n",
    "$\\frac{TP}{TP + FN}$\n",
    "\n",
    "(aka true positive rate) \n",
    "\n",
    "#### Accuracy: \n",
    "Informally, the fraction of predictions the model got right\n",
    "\n",
    "$\\frac{TP+TN}{TP+FP+TN+FN}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99455b2",
   "metadata": {},
   "source": [
    "## Bayes Theorem and its relevance to ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7678789e",
   "metadata": {},
   "source": [
    "#### Definition: \n",
    "$P(A|B) = \\frac{P(B|A)P(A)}{P(B} = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(A and B)}{P(B)}$\n",
    "\n",
    "#### Relevance to ML\n",
    "Bayes' rule relates conditional probabilities to joint probabilities, or, alternatively, the posterior probability given prior knowledge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5855f82c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e5bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
