{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17727ed0",
   "metadata": {},
   "source": [
    " My own answers to some Springboard questions : https://www.springboard.com/blog/data-science/machine-learning-interview-questions/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453ac3d7",
   "metadata": {},
   "source": [
    "## Explain the 'bias-variance' trade off\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ec913",
   "metadata": {},
   "source": [
    "The bias-variance trade off refers to two important aspects of an estimate's correctness. Bias measures accuracy of the estimate, while the variance of an estimate measures the precision of estimates over repeated attempts at estimation. They measure different aspects of the estimator (or statistic) and optimizing for one of bias or variance may impact the other detrimentally. \n",
    "\n",
    "Mean Squared Error can decompose into $MSE(\\hat{\\theta}) = Var_\\theta(\\hat{\\theta}) - Bias(\\theta, \\hat{\\theta})^2$, so using MSE loss takes both variance and bias into account. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de123448",
   "metadata": {},
   "source": [
    "## Performance metrics for classification tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf780a5",
   "metadata": {},
   "source": [
    "#### Precision: \n",
    "\n",
    "What proportion of positive identifications was actually correct?\n",
    "\n",
    "$\\frac{TP}{TP+FP}$\n",
    "\n",
    "(aka positive predictive value)\n",
    "\n",
    "#### Recall: \n",
    "What proportion of actual positives was identified correctly?\n",
    "\n",
    "$\\frac{TP}{TP + FN}$\n",
    "\n",
    "(aka true positive rate) \n",
    "\n",
    "#### Accuracy: \n",
    "Informally, the fraction of predictions the model got right\n",
    "\n",
    "$\\frac{TP+TN}{TP+FP+TN+FN}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99455b2",
   "metadata": {},
   "source": [
    "## Bayes Theorem and its relevance to ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7678789e",
   "metadata": {},
   "source": [
    "#### Definition: \n",
    "$P(A|B) = \\frac{P(B|A)P(A)}{P(B} = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(A and B)}{P(B)}$\n",
    "\n",
    "#### Relevance to ML\n",
    "Bayes' rule relates conditional probabilities to joint probabilities, or, alternatively, the posterior probability given prior knowledge. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1a1591",
   "metadata": {},
   "source": [
    "## How is knn diferent from k-means?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3056ddfd",
   "metadata": {},
   "source": [
    "## Explain how an ROC curve works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a91b12",
   "metadata": {},
   "source": [
    "## What is Bayes Theorem? How is it useful in a machine learning context?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702872a4",
   "metadata": {},
   "source": [
    "_Definition_: $$P(A|B) = \\frac{P(B|A)P(A)}{P(B)} = \\frac{P(A \\cap B)}{P(B)}= \\frac{P(A and B)}{P(B)}$$\n",
    "\n",
    "Bayes theorem gives theoretical framework to update a posterior distribution given a prior (information/distribution) and a likelihood based on observed data. \n",
    "\n",
    "$$posterior = \\frac{likelihood*prior}{\\textit{normalizing constant}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90da6a10",
   "metadata": {},
   "source": [
    "## What makes a naive Bayes classifier 'naive'?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2a59b4",
   "metadata": {},
   "source": [
    "The 'naive' assumption that is the foundation for the naive Bayes classifier is that each feature $X_i$ is conditionally independent of each of the remaining $X_i$'s, given the outcome, $Y$. This assumption totally ignores correlation between features, but allows us to simplify the the probability statement $P(X_1...X_n|Y)$ into a product $\\Pi_{i=1}^nP(X_i|Y)$. \n",
    "\n",
    "Formula for Bayes classifier (using Bayes' Rule) for k'th possible value is $$P(Y=y_k| X_1 ...X_n) = \\frac{P(Y=y_k)P(X_1...X_n|Y = y_k)}{\\sum_jP(Y=y_j)P(X_1...X_n|Y=y_j)}$$\n",
    "\n",
    "Applying the 'naive' simplification and abandoning conditional correlations in the data, this becomes:\n",
    "\n",
    "$$P(Y=y_k| X_1 ...X_n) = \\frac{P(Y=y_k)\\Pi_{i=1}^nP(X_i|Y=y_k)}{\\sum_jP(Y=y_j)\\Pi_{i=1}^nP(X_i|Y=Y_j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbbc42b",
   "metadata": {},
   "source": [
    "## Explain the difference between L1 and L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de9648",
   "metadata": {},
   "source": [
    "## What's the difference between Type 1 and Type II error?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200fa0fc",
   "metadata": {},
   "source": [
    "## What is deep learning and how does it contarst with other machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f7285",
   "metadata": {},
   "source": [
    "## What is the difference between a generative and a discriminative model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67f46d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad014f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fdf79c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e5bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
